# ❄️ INTRODUCTION 

## 𖠿 Snowflake Architecture Overview 
![image](https://github.com/user-attachments/assets/36615c05-c0ac-43a0-bc19-08a61c0ea1f2)    

**Elastic Parallel Processing (EPP)**
- Snowflake was the first data warehouse to introduce **EPP **
- Snowflake pioneered EPP: Decouples compute (processing power) from storage.
- Virtual warehouses act like “car engines”—they execute queries on data stored in schemas.
- Snowflake is 100% cloud-based—no local installation possible.

**Cloud service provider :** Snowflake runs only on AWS, Azure, GCP
### 🛠️ Services Layer 
Key built-in services include:
- Authentication **(basic, key pair, OAuth, SSO, MFA)**
- Infrastructure & metadata management
- Query parsing & optimization
- Access control (roles, users, privileges)
- Serverless compute (billed separately, ~$1.5 per use)

### 🖥 Compute Layer: Virtual Warehouses
- Core engine: SQL Engine
- Acts like a car: once it’s started, it consumes resources whether or not it's actively running queries.
- Query processing happens here
- Billing is per-second, but minimum of 1 minute is charged.
   
**🧩 Multi-Cluster Warehouses**
- Designed for concurrent users and parallel processing.
- Support automatic horizontal scaling (more clusters), not automatic size scaling (e.g., Small → Medium).
- Manual scaling is required for increasing size (e.g., from Large to X-Large).

### 🗄️Storage Layer
Data is stored in Snowflake-managed storage—not accessible at file-level.     
Access via:
- SQL queries
- SnowSight Web UI
Time Travel: Allows querying historical versions of data:
```SQL
"SELECT * FROM table AT(timestamp)"
```
- **Internal Stages:** Used for staging data during loads/unloads.
- 3 types:
    - User stage
    - Table stage
    - Named stage
- **External Stages:** External storage (like S3) integration.
- Inbound transfers are free. Outbound data transfer incurs cost.


## What is a Virtual Warehouse ? 
- Think of it as our **“compute engine”—no queries can run without it**.
- Snowflake automatically creates one warehouse for new users:
  - Size: X-Small
  - State: Initially suspended
- You’ll use this default warehouse for all your query executions.

## Cost and Sizing of Warehouses

- **X-Small** is the **smallest and cheapest** option:
  - Consumes 1 credit/hour (on a running basis).
  - Costs vary based on your edition and cloud provider:
      - **Enterprise Edition on AWS:** ~$3 per credit
      - **Standard Edition:** ~$2 per credit
- **Larger warehouse sizes** follow powers of two (2, 4, 8 nodes, etc.), using more credits.

### How Snowflake Pricing Works
- Snowflake charges per credit for compute, not per second or per minute:
  - e.g., X-Small (1 node) = 1 credit/hour
- **Your bill = warehouse size × runtime (in hours) × credit cost**
- Additional features like **Time Travel** or **Multi-Cluster** are hard to price directly, so compute usage is Snowflake’s main monetization model.

## Best Practice Configuration for Cost Optimization
Always enable these two checkboxes:      
#### 1. Auto-Resume:
- When a query is triggered and the warehouse is suspended, it automatically starts.
#### 2. Auto-Suspend:
- When the warehouse is idle, it shuts down automatically.
- **Best practice:** Set auto-suspend to **1 minute**.
  - Even if we set a smaller value like 10 seconds, **Snowflake internally polls every minute**, so we will be charged for one minutes.
> If auto-suspend is not enabled, you'll keep paying even when not using the warehouse.

## ⏳ Execution Time & Cost Analysis

Let's say a query took nearly **4 minutes** to execute on the **X-Large** warehouse.
Warehouse cost is calculated based on:
  - Credit usage per hour
  - Price per credit

**💰 Cost Breakdown:**
- X-Large warehouse = **128 credits/hour**
- Enterprise Edition = **$3/credit**
- Per-minute cost per node = **$3 ÷ 60 = $0.05**
- Total per-minute cost for 128 nodes = **128 × $0.05 = $6.40**
- Cost for 4 minutes of execution = **$6.40 × 4 = $25.60**

> Total Cost of Query Execution: **~$25.60**

## 🖧 Multi-Cluster Virtual Warehouse - Cost Analysis 
**Setup**
- Chose a 3X-Large warehouse (64 credits/hour)
- Enabled **multi-cluster mode** with:
  - **Minimum Clusters: 3**
  - **Maximum Clusters: 3**

Resulting in : **64 nodes × 3 clusters = 192 nodes total**   
Example: Let's say the warehouse was resumed, kept active for about 10 seconds, then manually suspended. No queries were executed during this time.

**💰 Cost Analysis**
- Key Pricing Details:
  - Snowflake charges by second, but with a 1-minute minimum per warehouse resume.
  - In Enterprise Edition, pricing is **$3 per credit**:
  - That equals $0.05 per node per minute.

- Final Cost Breakdown:
  - **192 nodes × $0.05 = $9.60 (rounded to ~$10)**

> ⚠️ Even without running a query, just resuming the warehouse for a few seconds would trigger this minimum 1-minute billing.

## ⚙️ Best Practices in Snowflake: Cost & Resource Management
### 🏎️ Virtual Warehouses (Your “Car Engines”)
- Always suspend unused warehouses to avoid credit consumption.
- Go to Admin > Warehouses and set auto-suspend = 1 minute.
- If a warehouse is idle for 10+ minutes without this setting, We'll be charged for 10 min.
- Delete unused large warehouses to prevent accidental reuse and excess cost.

### 💰 Cost Awareness in the SnowSight UI**
- SnowSight UI itself uses compute for dashboards like Cost Management.
   - Simply opening these dashboards can automatically start a warehouse and consume credits.
- Use dashboards efficiently and suspend warehouses immediately after viewing if not needed.

### 🧾 Query History and Auditing**
- All queries are logged, including:
   - User-run queries
   - System/background queries
- Use Query History to monitor who is consuming resources and when.

### 💾 Storage Best Practices**
- Review and monitor databases regularly from the Data tab.
- Snowflake provides sample databases that can be massive in size:
   - Example: An orders table with 1.5 billion rows, 48+ GB
   - Full database size: 100+ TB
- Avoid copying large databases or tables locally:
   - Snowflake is a cloud-native system—everything is remote.
   - Transferring large datasets out costs money and is inefficient.
> Instead of copying Use Zero-Copy Cloning to create test/dev datasets. Use data sharing features for collaboration across accounts.


