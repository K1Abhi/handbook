
## ‚ùÑÔ∏è Snowflake Data Pipelines 

### Scenario 1: Real-Time Table Syncing via Change Data Capture (CDC)

#### Use Case
- A `customers` table is owned by an external department.
- Need to maintain an up-to-date copy in our Snowflake database.
- Sync should reflect all changes in **near real-time**.

#### Solution Overview
- Use **Change Data Capture (CDC)** to handle incremental updates.

#### Methods for Implementing CDC in Snowflake
1. **MERGE Statement**
   - SQL command to upsert records into a target table.
2. **Change Tracking**
   - Allows tracking changes on a table without capturing the actual data.
3. **Streams and Tasks**
   - Earlier method to automate CDC workflows.
4. **Dynamic Tables**
   - New and simplified approach to manage incremental data changes.

---

### Scenario 2: Automated CSV Ingestion via Snowpipe

#### Use Case
- New employee records are added using CSV files dropped into an **Amazon S3 bucket**.
- Only insertions are allowed (no updates or deletions).

#### Solution Overview
- Use **Snowpipe** to automate CSV ingestion.

#### Components Involved
- **External Stage**: Connects Snowflake to the S3 bucket.
- **Snowpipe**: Automates continuous data loading from S3 into Snowflake.

---

### Summary

| Requirement                       | Solution                             |
|-----------------------------------|--------------------------------------|
| Sync external table in real-time  | Change Data Capture (CDC)            |
| Handle inserts via CSV in S3      | Snowpipe + External Stage (S3)       |

> These approaches help build robust, scalable, and near real-time data pipelines in Snowflake.
---
## Batch & Stream Data Transfer

## Overview

How to implement **batch and streaming data ingestion** from a transactional source system (typically OLTP) into Snowflake using **Change Data Capture (CDC)**.
## Typical Pipeline Flow

![image](https://github.com/user-attachments/assets/161ada8f-7a97-491a-9dc5-05ae763a503f)

1. **Initial Full Batch Transfer**
   - One-time full data load from the source database to Snowflake.
   - May take significant time depending on table size.

2. **Enable CDC on Source Database**
   - Supported by databases like PostgreSQL, Oracle, SQL Server, SAP, etc.
   - Captures changes (INSERTs, UPDATEs, DELETEs) in a **CDC log**.

3. **Periodic CDC Log Transfer**
   - A data transfer tool (e.g., **Fivetran, Airbyte, Stitch**) reads CDC logs.
   - These are loaded into a **stage table** in Snowflake.
   - Each record reflects an incremental change.
## Example Workflow
- **Stage Table** receives CDC records:
  - **Inserts**: New records.
  - **Updates**: Shown as DELETE of old + INSERT of new.
  - **Deletes**: Marked with a `DEL` flag.

---
### Illustration Example:
![image](https://github.com/user-attachments/assets/996a54c4-e3f7-44e2-abc2-2787aed81253)
- Stage Table:
  - Initial: 3 inserts (new records) 1st operation.
  - Next: 2 more rows (1 update to name, 1 delete marked via flag).
---

## Applying Changes in Snowflake

The core challenge: **How to reflect stage table changes into the target snapshot table.**

### CDC Methods in Snowflake:

1. **Manual Merge Statements**
   - Use a **stored procedure** to encapsulate a `MERGE` statement.
   - Applies changes from stage to target table.

2. **Change Tracking**
   - Enables tracking of changes at the table level.
   - Captures changes to be applied elsewhere.

3. **Streams and Tasks** *(Older Approach)*
   - **Stream** detects changes.
   - **Task** runs periodically and checks if `SYSTEM$STREAM_HAS_DATA`.
   - If changes exist, runs `MERGE` using metadata fields like `METADATA$ACTION`.

4. **Dynamic Tables** *(Modern & Recommended)*
   - Replaces streams and tasks.
   - Automatically manages incremental changes and updates to the target table.

---

## Summary Table

| Step                        | Description |
|-----------------------------|-------------|
| Initial Batch Load          | One-time full data transfer |
| CDC Log Capture             | Source DB logs changes |
| CDC Log Ingestion           | Transfer tools push changes to Snowflake |
| Change Application Methods  | Manual Merge, Change Tracking, Streams & Tasks, Dynamic Tables |

> Modern Snowflake CDC is best implemented using **Dynamic Tables** for simplicity and automation.



## üõ†Ô∏è Manual CDC in Snowflake using MERGE and Stored Procedures ‚öôÔ∏è

This walkthrough demonstrates a **manual Change Data Capture (CDC)** process in Snowflake using a stage (`source`) table and a `target` table. The CDC is implemented via a `MERGE` statement and encapsulated in a stored procedure.

### ‚öíÔ∏è Setup

#### 1. Create Database and Schema

```sql
CREATE DATABASE IF NOT EXISTS data_pipelines;
CREATE OR REPLACE SCHEMA data_pipelines.manual_cdc;
```

### ùÑú Create Tables

#### 2. Source and Target Tables

```sql
CREATE TABLE source (
  del BOOLEAN, 
  id INT, 
  name STRING
);

CREATE TABLE target (
  id INT, 
  name STRING
);
```

### üóÇÔ∏è CDC Merge Logic

#### 3. MERGE Statement

```sql
MERGE INTO target t 
USING source s 
ON t.id = s.id
WHEN NOT MATCHED AND NOT del THEN 
  INSERT (id, name) VALUES (s.id, s.name)
WHEN MATCHED AND del THEN 
  DELETE
WHEN MATCHED AND NOT del THEN 
  UPDATE SET t.name = s.name;
```

### ‚è© Stored Procedure for CDC

#### 4. Create Procedure

```sql
CREATE PROCEDURE cdc() RETURNS INT
AS $$
MERGE INTO target t USING source s ON t.id = s.id
  WHEN NOT MATCHED AND NOT del THEN 
    INSERT (id, name) VALUES (s.id, s.name)
  WHEN MATCHED AND del THEN 
    DELETE
  WHEN MATCHED AND NOT del THEN 
    UPDATE SET t.name = s.name;
$$;
```

### Test the Manual CDC

#### 5. Initial Inserts (3 rows)

```sql
INSERT INTO source VALUES (False, 1, 'John'), 
                         (False, 2, 'Mary'), 
                         (False, 3, 'George');

CALL cdc();
TRUNCATE TABLE source;
SELECT * FROM target;
```

Expected Result:
```
+----+--------+
| ID | NAME   |
+----+--------+
| 1  | John   |
| 2  | Mary   |
| 3  | George |
+----+--------+
```

#### 6. Update and Delete Test

```sql
-- ID 1: Update to 'Mark'
-- ID 2: Delete
INSERT INTO source VALUES (False, 1, 'Mark'), 
                         (True, 2, NULL);

CALL cdc();
TRUNCATE TABLE source;
SELECT * FROM target;
```

Expected Result:
```
+----+--------+
| ID | NAME   |
+----+--------+
| 1  | Mark   |
| 3  | George |
+----+--------+
```

### Notes

- `del` flag is used in the `source` table to indicate delete operations.
- Snowflake does **not enforce primary keys**, so logical uniqueness is assumed by `id`.
- This MERGE-based CDC is often encapsulated in procedures for easier scheduling and orchestration.
---

## ‚ùÑÔ∏è Snowflake CDC using CHANGE_TRACKING üìà

[Snowflake Documentation of CHANGE_TRACKING](https://docs.snowflake.com/en/sql-reference/constructs/changes)


#### Scenario
You have a typical snapshot table (no `del_flag`, no audit columns), and you want to emit all incremental changes `(INSERTs, UPDATEs, DELETEs)` from it.
Snowflake's `CHANGE_TRACKING` makes this possible without modifying the table schema.

##### 1. Set up the database and schema:
```sql
CREATE DATABASE IF NOT EXISTS data_pipelines;
CREATE OR REPLACE SCHEMA data_pipelines.change_tracking;
```
##### 2.Create a snapshot-style source table and enable change tracking:
```sql
CREATE OR REPLACE TABLE source(id INT, name STRING);
ALTER TABLE source SET CHANGE_TRACKING = TRUE;
```
##### 3.Capture a reference timestamp before changes:
```sql
SET ts1 = (SELECT CURRENT_TIMESTAMP());
```
##### 4. Perform data changes
```sql
-- Initial inserts
INSERT INTO source VALUES (1, 'John'), (2, 'Mary'), (3, 'George');

-- Update and delete
UPDATE source SET name = 'Mark' WHERE id = 1;
DELETE FROM source WHERE id = 2;
```
##### 5. Track and query changes:
The CHANGES clause enables querying the change tracking metadata for a table or view within a specified interval of time without having to create a stream with an explicit transactional offset.
In a query, the `CHANGES` clause is specified in the `FROM` clause.
- Only new inserts (append-only):
  ```sql
  SELECT * FROM source
  CHANGES (INFORMATION => APPEND_ONLY) AT (TIMESTAMP => $ts1);
  ```
- All changes (insert, update, delete):
  ```sql
  SELECT * FROM source
  CHANGES (INFORMATION => DEFAULT) AT (TIMESTAMP => $ts1);
  ```
  This result includes:
   - Metadata columns: _CHANGE_TYPE, _CHANGE_TIMESTAMP, _ROW_ID
   - `I = Insert`, `U = Update`, `D = Delete`

##### 6. Create a target table reflecting the current state from that point forward:
```sql
CREATE OR REPLACE TABLE target AS
  SELECT id, name FROM source
  CHANGES (INFORMATION => DEFAULT) AT (TIMESTAMP => $ts1);

SELECT * FROM target;
```

> Even if your source table doesn‚Äôt have audit columns or flags, you can still track all incremental changes using Snowflake‚Äôs `CHANGE_TRACKING` feature by simply enabling it and using the `CHANGES` clause in queries.

---
## Snowflake CDC Using Streams ‚ñ∂ and Tasks üìù 

##### What is CDC (Change Data Capture)?
Change Data Capture (CDC) is a method to track and respond to changes (INSERTs, UPDATEs, DELETEs) made in a database table. In Snowflake, this is efficiently handled using Streams to capture changes and Tasks to automate processing.

#####  üëâ Step-by-Step Implementation 

##### 1. Create Database and Schema
```sql
   CREATE DATABASE data_pipelines;
   CREATE SCHEMA streams_and_tasks;
```

##### 2. Create Source and Target Tables
`source`: Table where changes will occur (simulates an operational database).
`target`: Table where CDC changes will be applied (acts like a replica or data warehouse).

```sql
CREATE TABLE source (
  id INT,
  name STRING
);

CREATE TABLE target (
  id INT,
  name STRING
);
```
##### 3. Create a Stream on the Source Table
**Purpose**: Stream tracks all DML (Data Manipulation Language) changes on the source table from the moment it is created.
```sql
CREATE OR REPLACE STREAM mystream ON TABLE source;
```
- Captures `INSERTs`, `UPDATEs`, `DELETEs`
- Adds metadata fields like `METADATA$ACTION` and `METADATA$ISUPDATE`

##### 4. Insert Sample Data into Source Table
**Purpose**: Simulate a change in source data.
```sql
INSERT INTO source VALUES
  (1, 'John'),
  (2, 'Mary'),
  (3, 'George');
```
##### 5. Query the Stream to View Tracked Changes
**Purpose**: Verify that the stream is capturing changes from the source table.
```sql
SELECT * FROM mystream;
```
Output will show:
- Changed rows from source
- Action metadata (`INSERT`, `UPDATE`, `DELETE`)

##### 6. Create a Task to Automate Change Application
**Purpose**: A Task automates applying the changes from the stream to the target table using a scheduled job.
```sql
CREATE OR REPLACE TASK mytask
  WAREHOUSE = compute_wh
  SCHEDULE = '1 minute'  -- runs every minute
WHEN
  system$stream_has_data('mystream')  -- only runs if stream has new changes
AS
  MERGE INTO target t
  USING (
    SELECT * FROM mystream
  ) s
  ON t.id = s.id
  -- Handle DELETE
  WHEN MATCHED AND METADATA$ACTION = 'DELETE' AND METADATA$ISUPDATE = 'FALSE' THEN
    DELETE
  -- Handle UPDATE
  WHEN MATCHED AND METADATA$ACTION = 'INSERT' AND METADATA$ISUPDATE = 'TRUE' THEN
    UPDATE SET t.name = s.name
  -- Handle INSERT
  WHEN NOT MATCHED AND METADATA$ACTION = 'INSERT' THEN
    INSERT (id, name) VALUES (s.id, s.name);
```

##### 7. Resume the Task (Important!)
> Tasks are created in a suspended state by default.
```sql
ALTER TASK mytask RESUME;
```
##### 8. Execute the Task Manually (Optional)
We can force execution before the scheduled time.
```sql
EXECUTE TASK mytask;
```
##### 9. Verify Data in Target Table
Ensure the changes from the source table were applied correctly.
```sql
SELECT * FROM target;
```

##### 10. Make More Changes to the Source Table
Simulate updates and deletions:
```sql
UPDATE source SET name = 'Mark' WHERE id = 1;
DELETE FROM source WHERE id = 2;
```

##### ‚úÖ Run the Task Again and Re-Verify
Execute task manually (or wait for schedule):
```sql
EXECUTE TASK mytask;
```
Then check the target:
```sql
SELECT * FROM target;
```
##### üìä Monitor Task History
Use this to debug or verify task executions:
```sql
SELECT * FROM TABLE(information_schema.task_history())
ORDER BY scheduled_time DESC;
```
---
#### üìù Key Notes & Best Practices

| Concept                            | Description                                                                                 |
| ---------------------------------- | ------------------------------------------------------------------------------------------- |
| **Stream Limitation**              | Streams only capture changes after their creation.                                          |
| **METADATA\$ Columns**             | These include `METADATA$ACTION` `(INSERT, DELETE)` and `METADATA$ISUPDATE` `(TRUE for UPDATE)`. |
| **Use `system$stream_has_data()`** | Prevents task from running unnecessarily.                                                   |
| **Task Scheduling**                | Use `SCHEDULE = '1 minute'` or similar for automation.                                      |
| **Cost Tip**                       | Serverless tasks are 1.5x more expensive but auto-scale. Use with awareness.                |

---
## Snowflake CDC Using Dynamic Tables ‚ö°

**What are Dynamic Tables?**
Dynamic Tables are a newer, more encapsulated approach to implementing CDC in Snowflake. They automatically track and apply changes from a source table, eliminating the need for separate Streams and Tasks.
> **Key Benefit:** Less operational overhead‚Äîno need to create and manage tasks or merge logic manually.

#### Step-by-Step Implementation

##### 1. Create Database and Schema
```sql
CREATE DATABASE IF NOT EXISTS data_pipelines;
CREATE OR REPLACE SCHEMA data_pipelines.dynamic_tables;
```
##### 2. Create Source Tables 
```sql
CREATE TABLE source (
  id INT,
  name STRING
);
```
This is the base table where data changes (INSERT, UPDATE, DELETE) will occur.
##### 3. Create Source Tables 
```sql
CREATE DYNAMIC TABLE target
  WAREHOUSE = compute_wh  -- Required: used for periodic refresh
  TARGET_LAG = '1 minute' -- Refresh delay to sync with source
AS
  SELECT id, name FROM source;
```
**Key Components:**
- `WAREHOUSE`: Needed to run the internal background process that syncs the dynamic table.
- `TARGET_LAG`: Defines how frequently the target syncs with the source (like a built-in scheduler).
- `SELECT`: Defines the shape and transformation logic of the dynamic table. Here, it's a simple mirror.

##### 4. Insert Initial Data

```sql
INSERT INTO source VALUES 
  (1, 'John'),
  (2, 'Mary'),
  (3, 'George');
```
> Insert 3 records to simulate initial ingestion.

##### 5. Check the Target Table
```sql
SELECT * FROM target;
```
- Wait for about a minute (or whatever your `TARGET_LAG` is) for changes to reflect.
- Dynamic Table auto-applies the changes based on the `SELECT` logic.

##### 6. Simulate More Changes
```sql
UPDATE source SET name = 'Mark' WHERE id = 1;
DELETE FROM source WHERE id = 2;
```
> This simulates an `UPDATE` and `DELETE` event in the source table.

##### 7. Re-check the Target Table
```sql
SELECT * FROM target;
```
- After the target lag period, the target table reflects the updated source:
  - Updated name for `id = 1`
  - Deleted record for `id = 2` is no longer present

##### 8. Suspend the Dynamic Table (Optional)
```sql
ALTER DYNAMIC TABLE target SUSPEND;
```
- **Purpose:** Prevent unnecessary compute usage if this is only a demo or not needed actively.
- This suspends the internal background job associated with the dynamic table.

#### üîç Comparison: Streams + Tasks vs Dynamic Tables

| Feature                     | Streams + Tasks                         | Dynamic Tables                      |
| --------------------------- | --------------------------------------- | ----------------------------------- |
| Setup Complexity            | Medium (requires stream + task + merge) | Low (single DDL statement)          |
| Maintenance                 | Manual (task scheduling, merge logic)   | Automated                           |
| Custom Transformation Logic | Yes                                     | Yes                                 |
| Requires Warehouse          | Yes (for tasks)                         | Yes (for refresh execution)         |
| Cost Awareness              | Moderate control                        | May need to suspend when not in use |

#### üìë Summary Notes
- **Dynamic Tables = Simpler CDC Alternative** introduced in 2023.
- Automatically manages refreshes based on `TARGET_LAG`.
- Can be used with **transformations in the `SELECT` clause**.
- Needs a **warehouse** to function.
- Suspend it when not needed to avoid compute costs.
