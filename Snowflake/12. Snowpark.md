# Snowpark 

## What is Snowpark?
Snowpark is a client-side library, not a server-side engine or tool. It allows you to write code (especially in Python, Java, or Scala) that executes within Snowflake, close to the data. The main goal is to push business logic to where the data lives inside Snowflake’s compute infrastructure so we don’t need to move large amounts of data back and forth between client and server.

## Why is there a push for Snowpark?
- Snowflake initially supported stored procedures (SPs) with:
  - Only single SQL statements
  - And JavaScript (which was clunky for complex pipelines)
- Then came **Snowflake Scripting** (like PL/SQL or T-SQL), which allowed procedural SQL blocks.
- Now, Snowpark takes it a step further:
  - Let you define complex business logic (e.g., data transformations) using DataFrame APIs and Python functions
  - All while keeping the processing inside Snowflake, without pulling the data to a client

## Client vs. Server Execution
- Traditionally, logic would run on your local machine or cloud VM, connecting to Snowflake and pulling data to process.
- With Snowpark, we instead send your logic to Snowflake, and it’s executed on the same infrastructure where our data lives.
- This reduces latency, cost, and bandwidth usage.

![image](https://github.com/user-attachments/assets/b4cf48f2-74a3-46ae-83f8-880435d0c968)

##  Snowflake’s Goal with Snowpark 
Snowflake aims to turn its compute layer into a hybrid of:
- Database Server (as usual), and
- Application Server (by supporting business logic natively)
They already support:
- SQL scripting (procedures, loops, conditions)
- Java/Scala Snowpark functions (because Snowflake is built on Scala)
- Python Snowpark code (via sandboxed virtual environments)


## Snowpark for Python


![image](https://github.com/user-attachments/assets/b04acc1a-ab26-42cc-9dae-72806c4c83f5)
Two key capabilities:   
**1. DataFrame API**
- Works like Pandas or PySpark: you chain functions to build transformations.
- Uses lazy evaluation: queries only run when you call .collect(), .show(), etc.
- Behind the scenes, the DataFrame query is translated to SQL and executed by Snowflake’s SQL engine.

**2. Python Functions for Server Execution**
You can write:
- **Stored Procedures:** can run SQL and use logic (loops, API calls, etc.)
- **UDFs (User-Defined Functions):** used in SQL queries for row-level operations.
- **UDTFs (User-Defined Table Functions):** used when your function returns a table (multiple rows).

This Python code is:
- Serialized and sent to Snowflake’s virtual machines
- Executed in a sandboxed Python environment
- Can use:
  - Pre-installed Anaconda packages
  - Or your own packages, uploaded to a stage (like cloud storage)

### How it All Works – Under the Hood
- You build your logic using Snowpark’s Python library
- When executed:
  1. The DataFrame logic is translated to SQL via the Python connector
  2. The Python code (UDF/SP) is serialized, transferred, and executed in the Snowflake Python sandbox
  3. Optional packages can be preloaded (Anaconda) or user-supplied (via stages)

![image](https://github.com/user-attachments/assets/d63737a4-a69a-4d61-8f3c-4acde43ee681)


### Why Use Snowpark?
- Avoids moving data between client and Snowflake
- Reduces latency and cost
- Enables complex business logic in Python
- Fully integrates with Snowflake’s compute and security layers
- Empowers data engineers and scientists to work more effectively inside Snowflake itself

## Create Query with DataFrame API

### Traditional Snowflake Connector Approach (Pandas)
Use `snowflake.connector.connect()` with parameters like account, user, password, database, and schema.    
Execute an SQL query using `pandas.read_sql(sql, conn)`:
```sql
SELECT dname, SUM(sal)
FROM emp JOIN dept ON emp.deptno = dept.deptno
WHERE dname <> 'RESEARCH'
GROUP BY dname
ORDER BY dname;
```

This returns a **Pandas DataFrame** that displays department names and total salaries (excluding "RESEARCH").

### 📢 Transition to Snowpark 
- **Introduce Snowpark:** A powerful way to write SQL queries programmatically using a functional API style.
- Connect using a `Session.builder` object instead of `connect()`:
```python
from snowflake.snowpark import Session

pars = {
    "account": "XLB86271",
    "user": "cscutaru",
    "password": os.environ["SNOWSQL_PWD"],
    "database": "EMPLOYEES",
    "schema": "PUBLIC"
}
session = Session.builder.configs(pars).create()
```
### **Building the Query with Snowpark DataFrame API**
1. Load employee and department tables:
   ```python
   emps = session.table("EMP").select("DEPTNO", "SAL")
   depts = session.table("DEPT").select("DEPTNO", "DNAME")
   ```
2. Perform a **join:**
   ```python
   q = emps.join(depts, emps.deptno == depts.deptno)
   ```
3. Apply a **filter:**
   ```python
   q = q.filter(q.dname != 'RESEARCH')
   ```
4. Perform **aggregation** and **sorting**:
   ```python
     (q.select("DNAME", "SAL")
    .group_by("DNAME")
    .agg({"SAL": "sum"})
    .sort("DNAME")
    .show())
   ```

### Code Comparison

**1. Using Pandas**
```python
  sql = """
  select dname, sum(sal)
    from emp join dept on emp.deptno = dept.deptno
    where dname <> 'RESEARCH'
    group by dname
    order by dname;
  """
  df = pd.read_sql(sql, conn)
  print(df)
```
**2. Using Snowpark**
```python
emps = session.table("EMP").select("DEPTNO", "SAL")
depts = session.table("DEPT").select("DEPTNO", "DNAME")
q = emps.join(depts, emps.deptno == depts.deptno)
q = q.filter(q.dname != 'RESEARCH')
(q.select("DNAME", "SAL")
  .group_by("DNAME")
  .agg({"SAL": "sum"})
  .sort("DNAME")
  .show())
```

### Important Points 
**Lazy Evaluation:**    
- The DataFrame API builds the query in memory as a logical plan.
- No data is transferred until an action like `.show()` or `.collect()` is called.
- Only then is a **single SQL query generated and pushed to the Snowflake server**.
**Generated SQL:**
A complex SQL statement is auto-generated (visible in Snowflake’s Query History).
- Example includes subqueries, joins, filters, and LIMIT 10 added by default when calling .show().

### Reference Code 
**Client Demo :** 
```python
import snowflake.connector
import os, pandas as pd

# connect to Snowflake with Python Client
conn = snowflake.connector.connect(
    account="XLB86271",
    user="cscutaru",
    password=os.environ["SNOWSQL_PWD"],
    database="EMPLOYEES",
    schema="PUBLIC")

sql = """
select dname, sum(sal)
  from emp join dept on emp.deptno = dept.deptno
  where dname <> 'RESEARCH'
  group by dname
  order by dname;
"""
df = pd.read_sql(sql, conn)
print(df)
```

**Snowpark - Demo**
```python
# see https://medium.com/snowflake/how-to-create-a-complex-query-with-snowpark-dataframe-in-python-2d31b9e0961b

from snowflake.snowpark import Session
import os

# connect to Snowflake with Snowpark
pars = {
    "account": "XLB86271",
    "user": "cscutaru",
    "password": os.environ["SNOWSQL_PWD"],
    "database": "EMPLOYEES",
    "schema": "PUBLIC" }
session = Session.builder.configs(pars).create()

"""
select dname, sum(sal)
  from emp join dept on emp.deptno = dept.deptno
  where dname <> 'RESEARCH'
  group by dname
  order by dname;
"""
emps = (session.table("EMP").select("DEPTNO", "SAL"))
depts = (session.table("DEPT").select("DEPTNO", "DNAME"))
q = emps.join(depts, emps.deptno == depts.deptno)

q = q.filter(q.dname != 'RESEARCH')
(q.select("DNAME", "SAL")
  .group_by("DNAME")
  .agg({"SAL": "sum"})
  .sort("DNAME")
  .show())

"""
# generated SQL query (from snowflake query History):

SELECT "EMPNO", "DNAME"
FROM (
  SELECT *
  FROM (
    (SELECT "EMPNO" AS "EMPNO", "DEPTNO_E" AS "DEPTNO_E"
    FROM (SELECT "EMPNO", "DEPTNO" AS "DEPTNO_E" FROM EMP)) AS SNOWPARK_LEFT
    INNER JOIN 
      (SELECT "DEPTNO_D" AS "DEPTNO_D", "DNAME" AS "DNAME"
      FROM (SELECT "DEPTNO" AS "DEPTNO_D", "DNAME" FROM DEPT)) AS SNOWPARK_RIGHT
    ON ("DEPTNO_E" = "DEPTNO_D")))
    WHERE ("DNAME" != 'RESEARCH')
    LIMIT 10
"""

```

## Snowpark API object model
Core Concepts of Snowpark API

### 1. Session Object
- Everything begins with creating a Session using `getActiveSession()` or via builder parameters.
- The session represents the connection to Snowflake.
- From a session, you can:
  - Run raw SQL `(session.sql(...))`
  - Create DataFrames `(session.createDataFrame(...))`
  - Use `session.table(...)` to reference existing tables
  - Write DataFrames to tables or Pandas `(df.write, df.to_pandas())`

![image](https://github.com/user-attachments/assets/2dc1980c-3475-4987-8d95-f70cc50dfe6a)

### 2. DataFrame Class/Object
- The central abstraction in Snowpark for querying and manipulating data.
- Uses functional programming to chain transformations (like PySpark or pandas).
Methods like:
  - .filter(...) ➜ Adds WHERE clause
  - .sort(...) ➜ Adds ORDER BY clause
  - .select(...) ➜ Adds SELECT projection
  - .join(...) ➜ Adds JOIN with condition
  - .groupBy(...).agg(...) ➜ Adds GROUP BY and aggregation
    
These methods **do not execute anything immediately**—they build up the SQL **metadata** client-side.

![image](https://github.com/user-attachments/assets/fd2b064e-89f6-4ea9-8d4e-603cd9a998dd)    

**Note:**
Execution is lazy, meaning nothing is sent to the server until you call:
  - .show()
  - .collect()
  - .toPandas()
These **trigger the SQL generation and execution** on the Snowflake server.

### Other Important Classes
#### Grouping Functions 
The GROUP BY operations — like groupBy, pivot, cube, rollup — will return a **RelationalGroupedDataFrame** object. Pass a **GroupingSets** object to the `group_by_grouping_sets` method.   
Return another DataFrame object with one aggregate function call: `sum`, `avg`, `min`, `max` for one single column. Or `agg` for multiple columns, each with its own specific aggregate function call:           
![image](https://github.com/user-attachments/assets/4da733a1-7c63-4bcc-9ef7-d48e5dc0427d)

**Example:**
The following returns a list of rows for groups by column “a”, with SUM and MAX on column “b”:
```python
df.group_by("a").agg(
    sum("b").alias("sum_b"),
    max("b").alias("max_b")
  ).collect()
```

For the window functions, you create a `PARTITION BY` or `ORDER BY` clause — with the optional **ROWS BETWEEN** or **RANGE BETWEEN** clause — by calling static methods of the **Window** class. The resulting **WindowSpec** object can be further refined, and you pass the result as an argument to the over method, on a Column class instance, which usually is part of a DataFrame.    
A **CASE** statement can be built with the **CaseExpr** class, which is based on **Column** and instantiated by a call to the **when** function.

### 3. Input/Output Class
You can load data into a DataFrame from a Table object or **DataFrameReader** class — from a **CSV, JSON, XML, Parquet, ORC or AVRO** file. The write method returns a **DataFrameWrite** that you can use to save data in a table or **COPY TO** another location:

![image](https://github.com/user-attachments/assets/173abe75-1a47-4af3-8bae-4f3f359fe2f1)

### Create Query with DataFrame 
![image](https://github.com/user-attachments/assets/f09374d1-6b57-42b8-80cd-e8d8ce42f77d)

