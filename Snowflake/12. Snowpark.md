# Snowpark 

## What is Snowpark?
Snowpark is a client-side library, not a server-side engine or tool. It allows you to write code (especially in Python, Java, or Scala) that executes within Snowflake, close to the data. The main goal is to push business logic to where the data lives inside Snowflake’s compute infrastructure so we don’t need to move large amounts of data back and forth between client and server.

## Why is there a push for Snowpark?
- Snowflake initially supported stored procedures (SPs) with:
  - Only single SQL statements
  - And JavaScript (which was clunky for complex pipelines)
- Then came Snowflake Scripting (like PL/SQL or T-SQL), which allowed procedural SQL blocks.
- Now, Snowpark takes it a step further:
  - Let you define complex business logic (e.g., data transformations) using DataFrame APIs and Python functions
  - All while keeping the processing inside Snowflake, without pulling the data to a client

## Client vs. Server Execution
- Traditionally, logic would run on your local machine or cloud VM, connecting to Snowflake and pulling data to process.
- With Snowpark, you instead send your logic to Snowflake, and it’s executed on the same infrastructure where your data lives.
- This reduces latency, cost, and bandwidth usage.

![image](https://github.com/user-attachments/assets/b4cf48f2-74a3-46ae-83f8-880435d0c968)

##  Snowflake’s Goal with Snowpark 
Snowflake aims to turn its compute layer into a hybrid of:
- Database Server (as usual), and
- Application Server (by supporting business logic natively)
They already support:
- SQL scripting (procedures, loops, conditions)
- Java/Scala Snowpark functions (because Snowflake is built on Scala)
- Python Snowpark code (via sandboxed virtual environments)


## Snowpark for Python


![image](https://github.com/user-attachments/assets/b04acc1a-ab26-42cc-9dae-72806c4c83f5)
Two key capabilities:
**1. DataFrame API**
- Works like Pandas or PySpark: you chain functions to build transformations.
- Uses lazy evaluation: queries only run when you call .collect(), .show(), etc.
- Behind the scenes, the DataFrame query is translated to SQL and executed by Snowflake’s SQL engine.

**2. Python Functions for Server Execution**
You can write:
- ** Stored Procedures:** can run SQL and use logic (loops, API calls, etc.)
- **UDFs (User-Defined Functions):** used in SQL queries for row-level operations.
- **UDTFs (User-Defined Table Functions):** used when your function returns a table (multiple rows).

This Python code is:
- Serialized and sent to Snowflake’s virtual machines
- Executed in a sandboxed Python environment
- Can use:
  - Pre-installed Anaconda packages
  - Or your own packages, uploaded to a stage (like cloud storage)

### How it All Works – Under the Hood
- You build your logic using Snowpark’s Python library
- When executed:
  1. The DataFrame logic is translated to SQL via the Python connector
  2. The Python code (UDF/SP) is serialized, transferred, and executed in the Snowflake Python sandbox
  3. Optional packages can be preloaded (Anaconda) or user-supplied (via stages)

![image](https://github.com/user-attachments/assets/d63737a4-a69a-4d61-8f3c-4acde43ee681)


### Why Use Snowpark?
- Avoids moving data between client and Snowflake
- Reduces latency and cost
- Enables complex business logic in Python
- Fully integrates with Snowflake’s compute and security layers
- Empowers data engineers and scientists to work more effectively inside Snowflake itself




