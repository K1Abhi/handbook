> **Note:** Querying **file metadata** in a stage does **not require a warehouse**, but querying **table data** in Snowflake **does require a warehouse**.

# üì• Loading CSV into Snowflake ‚ùÑÔ∏è

CSV files from your local system can be uploaded to Snowflake by first loading them into a **stage**.

### üóÇÔ∏è Types of Stages

1. **Internal Stages**:
   - **Named Stage**: Created explicitly by the user (`CREATE STAGE mystage`)
   - **User Stage**: Automatically created for each Snowflake user
   - **Table Stage**: Associated with each table

2. **External Stages**:
   - Integration with cloud storage like **AWS S3**, **Google Cloud Storage**, or **Azure Blob Storage**

---

### Example: Using an Internal Stage

```sql
-- Create a named internal stage
CREATE STAGE mystage;

-- List files uploaded to the stage
LIST @mystage;

-- we can query the contents and metadata of a staged file directly

SELECT metadata$file_row_number, $1, $2
FROM @mystage/emp.csv;
```

#### What This Query Does

- **`@mystage/emp.csv`**  
  Refers to the `emp.csv` file stored in the `mystage` stage. The `@` symbol indicates a reference to a stage.

- **`$1`, `$2`**  
  Positional column references used when querying structured files like CSVs without a defined schema:  
  - `$1` = First column in the file  
  - `$2` = Second column in the file

- **`metadata$file_row_number`**  
  A special Snowflake **metadata column** that returns the **row number** of the current record in the file. Useful for debugging or auditing file contents.

---

#### üì§ Query Output

The query will return:
1. The **row number** from the CSV file  
2. The **first column** value  
3. The **second column** value

---
---
### Inferring Schema from a CSV File in Snowflake
Snowflake allows you to automatically infer the schema of a file in a stage using the `INFER_SCHEMA` table function.

####  Step 1: Create a File Format

```sql
CREATE FILE FORMAT myfileformat
    TYPE = 'CSV'
    PARSE_HEADER = TRUE;
```
- `TYPE = 'CSV'`: Specifies the format type.
- `PARSE_HEADER = TRUE`: Indicates the file has a header row.

#### Step 2: Infer Schema from the File

#### Syntax Example
```sql
SELECT * 
FROM TABLE(INFER_SCHEMA(
    LOCATION => '@mystage',
    FILES => 'emp.csv',
    FILE_FORMAT => 'myfileformat'
));
```
#### Why Use `* FROM TABLE(...)` with `INFER_SCHEMA` in Snowflake?

In Snowflake, when using `INFER_SCHEMA`, you must wrap it inside `TABLE(...)` because it's a **table function**, not a scalar or regular function.

---

### What is a Table Function?

A **table function** returns tabular data ‚Äî that is, rows and columns ‚Äî just like a table.  
In order to query from it, you must use the `TABLE()` keyword in SQL.

#### Explanation
- `INFER_SCHEMA(...)`
This function analyzes the structure of the file and returns a result set (columns like `COLUMN_NAME`, `TYPE`, `NULLABLE`, etc.).

- `TABLE(INFER_SCHEMA(...))`
This tells Snowflake to treat the output of the function like a virtual table so you can query it.

- `SELECT *`
Selects all the columns returned by the `INFER_SCHEMA` function.

---

### üõ†Ô∏è Auto-Generate Column Descriptions in Snowflake

After inferring a schema using `INFER_SCHEMA`, you can automatically generate a `CREATE TABLE`-like column definition using `GENERATE_COLUMN_DESCRIPTION`.

---

### üßæ Query Example

```sql
SELECT GENERATE_COLUMN_DESCRIPTION (
  ARRAY_AGG(OBJECT_CONSTRUCT(*)), 'table'
) AS columns
FROM TABLE (INFER_SCHEMA(
    LOCATION => '@mystage',
    FILES => 'emp.csv',
    FILE_FORMAT => 'myfileformat'
));
```

#### What This Query Does
- `INFER_SCHEMA(...)`
Reads the file `emp.csv` from the stage `@mystage` using the file format myfileformat and returns rows that describe the file‚Äôs schema (column name, type, etc.).

- `OBJECT_CONSTRUCT(*)`
Converts each row from INFER_SCHEMA into a key-value object. Each object represents a column's metadata.

- `ARRAY_AGG(...)`
Aggregates all those objects into an array (so we can process them as a group).

- `GENERATE_COLUMN_DESCRIPTION(...)`
Takes the array of column metadata and generates a Snowflake-compatible list of column definitions ‚Äî like you'd use in a `CREATE TABLE` statement.

#### Result 
```JSON
"EMPNO" NUMBER(4, 0),
"ENAME" TEXT,
"JOB" TEXT,
"MGR" NUMBER(4, 0),
"HIREDATE" DATE,
"SAL" NUMBER(5, 1),
"COMM" NUMBER(5, 1),
"DEPTNO" NUMBER(2, 0)
```

### üèóÔ∏è Create Table Using Inferred Schema Template in Snowflake

Snowflake provides a simple and powerful way to create a table using the schema inferred from staged files (like CSVs). This is done using the `CREATE TABLE ... USING TEMPLATE` syntax.

---

### üßæ Query Example

```sql
CREATE TABLE emp USING TEMPLATE (
  SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))
  FROM TABLE (INFER_SCHEMA(
      LOCATION => '@mystage',
      FILES => 'emp.csv',
      FILE_FORMAT => 'myfileformat'
  ))
);
```

### üß† What This Query Does

#### `INFER_SCHEMA(...)`
- Infers the column names, data types, and nullability from the file `emp.csv` in stage `@mystage` using the specified `myfileformat`.

#### `OBJECT_CONSTRUCT(*)`
- Converts each inferred row (column definition) into a key-value object.

#### `ARRAY_AGG(...)`
- Aggregates all column definition objects into a single array.

#### `USING TEMPLATE (...)`
- Uses the array of column definitions to automatically generate and create the `emp` table with the inferred structure.

---

### ‚úÖ Benefits

- üö´ Eliminates manual typing of `CREATE TABLE` DDL  
- ‚úÖ Ensures the table schema exactly matches the source file  
- ‚öôÔ∏è Ideal for dynamic and automated data pipelines  

---

### üìå Result

The `emp` table will be created with the following characteristics:

- Column names inferred from the file‚Äôs header row  
- Appropriate data types assigned based on the file content  
- Columns marked as nullable or not, based on inferred schema  

---
---
> **Note:** # When using `INFER_SCHEMA`, it‚Äôs difficult to manually add constraints like: Data types with precision, Column sizes, Constriants like `PRIMARY KEY`, `NOT NULL`, `UNIQUE`.
> 
To simplify this process, use `GET_DDL` to extract the table definition and then modify it.

### üîç Get the Auto-Inferred Table DDL

```sql
SELECT GET_DDL('table', 'emp');
```
Which will give output as 

```sql
CREATE OR REPLACE TABLE EMP (
    EMPNO NUMBER(4,0),
    ENAME VARCHAR(16777216),
    JOB VARCHAR(16777216),
    MGR NUMBER(4,0),
    HIREDATE DATE,
    SAL NUMBER(5,1),
    COMM NUMBER(5,1),
    DEPTNO NUMBER(2,0)
);
```
We can now edit this output to match your exact requirements:
```sql
CREATE OR REPLACE TABLE EMP (
    EMPNO NUMBER(4,0) PRIMARY KEY,
    ENAME VARCHAR(20) NOT NULL UNIQUE,
    JOB VARCHAR(20),
    MGR NUMBER(4,0),
    HIREDATE DATE,
    SAL FLOAT,
    COMM FLOAT,
    DEPTNO NUMBER(2,0) NOT NULL
);
```
This version includes:
- `PRIMARY KEY` on `EMPNO`
- `UNIQUE` and `NOT NULL` on `ENAME`
- Trimmed column sizes for `VARCHAR`
- Nullable constraints set explicitly

### üì§ Load Data from Stage into Table
```sql
COPY INTO emp
FROM @mystage
FILES = ('emp.csv')
FILE_FORMAT = (FORMAT_NAME = 'myfileformat')
MATCH_BY_COLUMN_NAME = CASE_SENSITIVE
FORCE = TRUE;
```



#### `COPY INTO` Parameters 
| Parameter                    | Description                                                                                      | Example                                                              |
|-----------------------------|--------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|
| `FROM`                      | Specifies the stage or location to copy data from.                                               | `FROM @mystage`                                                     |
| `FILES`                     | Specifies specific files to load from the stage.                                                 | `FILES = ('emp.csv')`                                               |
| `FILE_FORMAT`               | Specifies the file format to use when parsing the file.                                          | `FILE_FORMAT = (FORMAT_NAME = 'myfileformat')`                      |
| `PATTERN`                   | Regex pattern to match files to load.                                                            | `PATTERN='.*.csv'`                                                  |
| `ON_ERROR`                  | Determines behavior when errors occur during loading.                                            | `ON_ERROR = 'CONTINUE'`                                             |
| `FORCE`                     | If `TRUE`, reloads files even if they were already loaded.                                       | `FORCE = TRUE`                                                      |
| `MATCH_BY_COLUMN_NAME`      | Aligns file columns to table columns by name. Can be `CASE_SENSITIVE`, `CASE_INSENSITIVE`, etc. | `MATCH_BY_COLUMN_NAME = CASE_SENSITIVE`                             |
| `VALIDATION_MODE`           | Checks file validity without loading data. Options: `RETURN_n_ROWS`, `RETURN_ALL_ERRORS`        | `VALIDATION_MODE = 'RETURN_2_ROWS'`                                 |
| `PURGE`                     | If `TRUE`, removes the file from the stage after successful load.                                | `PURGE = TRUE`                                                      |
| `RETURN_FAILED_ONLY`        | When validating, returns only the failed rows.                                                   | `RETURN_FAILED_ONLY = TRUE`                                         |
| `TRUNCATECOLUMNS`           | If `TRUE`, truncates strings that exceed column length instead of failing.                       | `TRUNCATECOLUMNS = TRUE`                                            |
| `ERROR_ON_COLUMN_COUNT_MISMATCH` | Throws error if number of columns in file doesn't match table.                                   | `ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE`                             |
| `NULL_IF`                   | Specifies string(s) in the file to interpret as NULL.                                            | `NULL_IF = ('\\N', 'NULL')`                                         |
| `ENCODING`                  | Character encoding of the file.                                                                  | `ENCODING = 'UTF8'`                                                 |
| `SINGLE`                    | If `TRUE`, treats the file as a single column (useful for semi-structured data).                 | `SINGLE = TRUE`                                                     |

> üîπ **Tip:** You can combine many of these parameters inside `FILE_FORMAT = (...)` or specify them directly in the `COPY INTO` command depending on your use case.


## üåê Creating an External Stage with Amazon S3 in Snowflake

### ü™£ Step 1: Create an S3 Bucket and Upload Files

- Create an S3 bucket: `my_snow_bucket`
- Upload CSV files into a folder/prefix, for example: `my_snow_bucket/data/`
  - Files: `emp2.csv`, `emp3.csv`

---

### üîê Step 2: Add an S3 Bucket Policy

Update the bucket policy to allow Snowflake access:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:GetObjectVersion",
        "s3:DeleteObject",
        "s3:DeleteObjectVersion"
      ],
      "Resource": "arn:aws:s3:::<bucket>/<prefix>/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetBucketLocation"
      ],
      "Resource": "arn:aws:s3:::<bucket>",
      "Condition": {
        "StringLike": {
          "s3:prefix": [
            "<prefix>/*"
          ]
        }
      }
    }
  ]
}
```
> üî∏Replace `<bucket>` with your bucket name (e.g., `my_snow_bucket`)
> üî∏Replace `<prefix>` with your folder (e.g., `data`)

### üë§Step 3: Create an AWS User & Access Credentials
- In AWS IAM, create a new user.
- Assign the above bucket policy to this user.
- Generate `Access Key ID` and `Secret Access Key` under `Security Credentials`.

### ‚ùÑÔ∏èStep 4: Create the Snowflake External Stage
```SQL
CREATE STAGE my_stage_s3
  URL = 's3://my_snow_bucket/data/'
  CREDENTIALS = (
    AWS_KEY_ID = 'your_access_key_id',
    AWS_SECRET_KEY = 'your_secret_access_key'
  );

-- LIST @my_stage_s3;
```
Now we can use `COPY INTO` to copy data into our tables 


## üìä Loading and Unloading Data in Snowflake

Below diagram shows the flow of data loading and unloading in Snowflake:

![Loading & Unloading in Snowflake](https://raw.githubusercontent.com/K1Abhi/handbook/main/images/snowflake/loading_unloading_snow.png)

